{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n",
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the string 'empty' to empty strings\n",
    "train = train.fillna('empty')\n",
    "test = test.fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text, remove_stopwords=False, stem_words=False):\n",
    "    \n",
    "    # Convert words to lower case \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\0k \", \"0000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" america \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text)\n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"os\", \"operating system\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"america\", text)\n",
    "    text = re.sub(r\" J K \", \" jk \", text)\n",
    "    \n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_questions(question_list, questions, question_list_name, dataframe):\n",
    "    #transform questions and display progress\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_word_list(question))\n",
    "        if len(question_list) % 100000 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_question1 is 24.7% complete.\n",
      "train_question1 is 49.5% complete.\n",
      "train_question1 is 74.2% complete.\n",
      "train_question1 is 98.9% complete.\n"
     ]
    }
   ],
   "source": [
    "train_question1 = []\n",
    "process_questions(train_question1, train.question1, 'train_question1', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_question2 is 24.7% complete.\n",
      "train_question2 is 49.5% complete.\n",
      "train_question2 is 74.2% complete.\n",
      "train_question2 is 98.9% complete.\n"
     ]
    }
   ],
   "source": [
    "train_question2 = []\n",
    "process_questions(train_question2, train.question2, 'train_question2', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_question1 is 4.3% complete.\n",
      "test_question1 is 8.5% complete.\n",
      "test_question1 is 12.8% complete.\n",
      "test_question1 is 17.1% complete.\n",
      "test_question1 is 21.3% complete.\n",
      "test_question1 is 25.6% complete.\n",
      "test_question1 is 29.8% complete.\n",
      "test_question1 is 34.1% complete.\n",
      "test_question1 is 38.4% complete.\n",
      "test_question1 is 42.6% complete.\n",
      "test_question1 is 46.9% complete.\n",
      "test_question1 is 51.2% complete.\n",
      "test_question1 is 55.4% complete.\n",
      "test_question1 is 59.7% complete.\n",
      "test_question1 is 63.9% complete.\n",
      "test_question1 is 68.2% complete.\n",
      "test_question1 is 72.5% complete.\n",
      "test_question1 is 76.7% complete.\n",
      "test_question1 is 81.0% complete.\n",
      "test_question1 is 85.3% complete.\n",
      "test_question1 is 89.5% complete.\n",
      "test_question1 is 93.8% complete.\n",
      "test_question1 is 98.0% complete.\n"
     ]
    }
   ],
   "source": [
    "test_question1 = []\n",
    "process_questions(test_question1, test.question1, 'test_question1', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_question2 is 4.3% complete.\n",
      "test_question2 is 8.5% complete.\n",
      "test_question2 is 12.8% complete.\n",
      "test_question2 is 17.1% complete.\n",
      "test_question2 is 21.3% complete.\n",
      "test_question2 is 25.6% complete.\n",
      "test_question2 is 29.8% complete.\n",
      "test_question2 is 34.1% complete.\n",
      "test_question2 is 38.4% complete.\n",
      "test_question2 is 42.6% complete.\n",
      "test_question2 is 46.9% complete.\n",
      "test_question2 is 51.2% complete.\n",
      "test_question2 is 55.4% complete.\n",
      "test_question2 is 59.7% complete.\n",
      "test_question2 is 63.9% complete.\n",
      "test_question2 is 68.2% complete.\n",
      "test_question2 is 72.5% complete.\n",
      "test_question2 is 76.7% complete.\n",
      "test_question2 is 81.0% complete.\n",
      "test_question2 is 85.3% complete.\n",
      "test_question2 is 89.5% complete.\n",
      "test_question2 is 93.8% complete.\n",
      "test_question2 is 98.0% complete.\n"
     ]
    }
   ],
   "source": [
    "test_question2 = []\n",
    "process_questions(test_question2, test.question2, 'test_question2', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting is complete.\n",
      "train_question1 is complete.\n",
      "train_question2 is complete\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the words for all of the questions\n",
    "all_questions = train_question1 + train_question2 \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "print(\"Fitting is complete.\")\n",
    "train_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)\n",
    "print(\"train_question1 is complete.\")\n",
    "train_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)\n",
    "print(\"train_question2 is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85104 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_question1 is complete.\n",
      "test_question2 is complete.\n"
     ]
    }
   ],
   "source": [
    "test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\n",
    "print(\"test_question1 is complete.\")\n",
    "test_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\n",
    "print(\"test_question2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_q1 is complete.\n",
      "train_q2 is complete.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_question_len = 30\n",
    "\n",
    "train_q1 = pad_sequences(train_question1_word_sequences, maxlen = max_question_len)\n",
    "print(\"train_q1 is complete.\")\n",
    "\n",
    "train_q2 = pad_sequences(train_question2_word_sequences, maxlen = max_question_len)\n",
    "print(\"train_q2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_q1 is complete.\n",
      "test_q2 is complete.\n"
     ]
    }
   ],
   "source": [
    "test_q1 = pad_sequences(test_question1_word_sequences, maxlen = max_question_len)\n",
    "print(\"test_q1 is complete.\")\n",
    "\n",
    "test_q2 = pad_sequences(test_question2_word_sequences, maxlen = max_question_len)\n",
    "print(\"test_q2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR =  '../glove/'\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to use 300 for embedding dimensions to match GloVe's vectors.\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 25697\n"
     ]
    }
   ],
   "source": [
    "nb_words = len(word_index)\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "\n",
    "# Initialize weights and biases for the Dense layers\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "bias = bias_initializer='zeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(nb_words+1,\n",
    "                            embedding_dim,\n",
    "                            weights=[word_embedding_matrix],\n",
    "                            input_length=max_question_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout,  Merge, Activation, LSTM,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1 = Input(shape=(max_question_len,))\n",
    "question2 = Input(shape=(max_question_len,))\n",
    "\n",
    "q1 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question1)\n",
    "q1 = LSTM(256)(q1)\n",
    "\n",
    "q2 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question2)\n",
    "q2 = LSTM(256)(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 30, 300)       25531500    input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 30, 300)       25531500    input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 256)           570368      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 256)           570368      embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 512)           0           lstm_3[0][0]                     \n",
      "                                                                   lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 256)           131328      concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             257         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 52,335,321\n",
      "Trainable params: 1,272,321\n",
      "Non-trainable params: 51,063,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343646 samples, validate on 60644 samples\n",
      "Epoch 1/3\n",
      "343646/343646 [==============================] - 3652s - loss: 0.4879 - acc: 0.7564 - val_loss: 0.4305 - val_acc: 0.7905\n",
      "Epoch 2/3\n",
      "343646/343646 [==============================] - 3584s - loss: 0.3921 - acc: 0.8143 - val_loss: 0.4062 - val_acc: 0.8069\n",
      "Epoch 3/3\n",
      "343646/343646 [==============================] - 3580s - loss: 0.3142 - acc: 0.8574 - val_loss: 0.4022 - val_acc: 0.8157\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_q1, train_q2],\n",
    "                    y_train,\n",
    "                    batch_size=64, \n",
    "                    epochs=3, \n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1 = Input(shape=(max_question_len,))\n",
    "question2 = Input(shape=(max_question_len,))\n",
    "\n",
    "q1 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question1)\n",
    "q1 = TimeDistributed(Dense(embedding_dim, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(q1)\n",
    "\n",
    "q2 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question2)\n",
    "q2 = TimeDistributed(Dense(embedding_dim, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model1 = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 30, 300)       25531500    input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 30, 300)       25531500    input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 300)       90300       embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 30, 300)       90300       embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 300)           0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 300)           0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 600)           0           lambda_1[0][0]                   \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           153856      concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             257         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 51,397,713\n",
      "Trainable params: 334,713\n",
      "Non-trainable params: 51,063,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343646 samples, validate on 60644 samples\n",
      "Epoch 1/3\n",
      "343646/343646 [==============================] - 937s - loss: 0.4991 - acc: 0.7503 - val_loss: 0.4547 - val_acc: 0.7769\n",
      "Epoch 2/3\n",
      "343646/343646 [==============================] - 944s - loss: 0.4337 - acc: 0.7898 - val_loss: 0.4246 - val_acc: 0.7943\n",
      "Epoch 3/3\n",
      "343646/343646 [==============================] - 937s - loss: 0.3959 - acc: 0.8117 - val_loss: 0.4136 - val_acc: 0.8010\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit([train_q1, train_q2],\n",
    "                    y_train,\n",
    "                    batch_size=28, \n",
    "                    epochs=3, \n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1 = Input(shape=(max_question_len,))\n",
    "question2 = Input(shape=(max_question_len,))\n",
    "\n",
    "q1 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question1)\n",
    "q1 = LSTM(256)(q1)\n",
    "\n",
    "q2 = Embedding(nb_words+1,embedding_dim,weights=[word_embedding_matrix],input_length=max_question_len,trainable=False)(question2)\n",
    "q2 = LSTM(256)(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model2 = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 30, 300)       25531500    input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 30, 300)       25531500    input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 256)           570368      embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 256)           570368      embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 512)           0           lstm_5[0][0]                     \n",
      "                                                                   lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 256)           131328      concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 256)           1024        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 256)           65792       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 256)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 256)           1024        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 128)           32896       batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 128)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 128)           512         dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 128)           16512       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 128)           512         dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             129         batch_normalization_4[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 52,453,465\n",
      "Trainable params: 1,388,929\n",
      "Non-trainable params: 51,064,536\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343646 samples, validate on 60644 samples\n",
      "Epoch 1/3\n",
      "343646/343646 [==============================] - 4369s - loss: 0.5486 - acc: 0.7218 - val_loss: 0.4954 - val_acc: 0.7579\n",
      "Epoch 2/3\n",
      "343646/343646 [==============================] - 4341s - loss: 0.4814 - acc: 0.7697 - val_loss: 0.4659 - val_acc: 0.7746\n",
      "Epoch 3/3\n",
      "343646/343646 [==============================] - 4393s - loss: 0.4292 - acc: 0.8021 - val_loss: 0.4378 - val_acc: 0.7916\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit([train_q1, train_q2],\n",
    "                    y_train,\n",
    "                    batch_size=32, \n",
    "                    epochs=3, \n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "model1.save_weights(\"model1.h5\")\n",
    "model2.save_weights(\"model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
